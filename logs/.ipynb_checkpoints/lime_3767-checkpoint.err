Map:   0%|                                                                       | 0/7540 [00:00<?, ? examples/s]Map:  73%|█████████████████████████████████████████▉               | 5541/7540 [00:00<00:00, 55188.47 examples/s]Map: 100%|█████████████████████████████████████████████████████████| 7540/7540 [00:00<00:00, 54718.79 examples/s]
Map:   0%|                                                                       | 0/5028 [00:00<?, ? examples/s]Map: 100%|█████████████████████████████████████████████████████████| 5028/5028 [00:00<00:00, 56856.28 examples/s]
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Map:   0%|                                                                       | 0/7540 [00:00<?, ? examples/s]Map:  13%|███████▋                                                  | 1000/7540 [00:00<00:05, 1268.48 examples/s]Map:  27%|███████████████▍                                          | 2000/7540 [00:01<00:03, 1714.66 examples/s]Map:  40%|███████████████████████                                   | 3000/7540 [00:01<00:02, 1917.32 examples/s]Map:  53%|██████████████████████████████▊                           | 4000/7540 [00:02<00:02, 1257.93 examples/s]Map:  66%|██████████████████████████████████████▍                   | 5000/7540 [00:03<00:01, 1294.74 examples/s]Map:  80%|██████████████████████████████████████████████▏           | 6000/7540 [00:04<00:01, 1517.93 examples/s]Map:  93%|█████████████████████████████████████████████████████▊    | 7000/7540 [00:04<00:00, 1463.80 examples/s]Map: 100%|██████████████████████████████████████████████████████████| 7540/7540 [00:05<00:00, 1452.88 examples/s]Map: 100%|██████████████████████████████████████████████████████████| 7540/7540 [00:05<00:00, 1455.80 examples/s]
Map:   0%|                                                                       | 0/5028 [00:00<?, ? examples/s]Map:  20%|███████████▌                                              | 1000/5028 [00:00<00:01, 2267.65 examples/s]Map:  40%|███████████████████████                                   | 2000/5028 [00:00<00:01, 2260.14 examples/s]Map:  60%|██████████████████████████████████▌                       | 3000/5028 [00:01<00:00, 2242.48 examples/s]Map:  80%|██████████████████████████████████████████████▏           | 4000/5028 [00:01<00:00, 2262.69 examples/s]Map:  99%|█████████████████████████████████████████████████████████▋| 5000/5028 [00:02<00:00, 1826.33 examples/s]Map: 100%|██████████████████████████████████████████████████████████| 5028/5028 [00:02<00:00, 1997.37 examples/s]
/Users/923673423/lime/environments/lime_env_39/lib64/python3.9/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/Users/923673423/lime/daproject/scripts/llm-classification-model.py:80: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|                                                                                    | 0/354 [00:00<?, ?it/s]/Users/923673423/lime/environments/lime_env_39/lib64/python3.9/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
  0%|▏                                                                           | 1/354 [00:03<19:04,  3.24s/it]  1%|▍                                                                           | 2/354 [00:03<10:15,  1.75s/it]  1%|▋                                                                           | 3/354 [00:04<08:18,  1.42s/it]  1%|▊                                                                           | 4/354 [00:05<06:38,  1.14s/it]  1%|█                                                                           | 5/354 [00:06<06:22,  1.10s/it]  2%|█▎                                                                          | 6/354 [00:07<05:36,  1.03it/s]  2%|█▌                                                                          | 7/354 [00:08<05:06,  1.13it/s]  2%|█▋                                                                          | 8/354 [00:09<05:11,  1.11it/s]  3%|█▉                                                                          | 9/354 [00:09<04:51,  1.18it/s]  3%|██                                                                         | 10/354 [00:10<05:09,  1.11it/s]                                                                                                                   3%|██                                                                         | 10/354 [00:10<05:09,  1.11it/s]  3%|██▎                                                                        | 11/354 [00:11<04:49,  1.18it/s]  3%|██▌                                                                        | 12/354 [00:12<04:36,  1.24it/s]  4%|██▊                                                                        | 13/354 [00:13<04:54,  1.16it/s]  4%|██▉                                                                        | 14/354 [00:13<04:39,  1.22it/s]  4%|███▏                                                                       | 15/354 [00:14<04:27,  1.27it/s]  5%|███▍                                                                       | 16/354 [00:15<04:20,  1.30it/s]  5%|███▌                                                                       | 17/354 [00:16<04:43,  1.19it/s]  5%|███▊                                                                       | 18/354 [00:17<04:30,  1.24it/s]