Map:   0%|          | 0/7540 [00:00<?, ? examples/s]Map:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 5683/7540 [00:00<00:00, 56624.85 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7540/7540 [00:00<00:00, 55927.91 examples/s]
Map:   0%|          | 0/5028 [00:00<?, ? examples/s]Map:   8%|â–Š         | 417/5028 [00:00<00:03, 1350.25 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5028/5028 [00:00<00:00, 12670.08 examples/s]
Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Map:   0%|          | 0/7540 [00:00<?, ? examples/s]Map:  13%|â–ˆâ–Ž        | 1000/7540 [00:00<00:01, 5968.72 examples/s]Map:  27%|â–ˆâ–ˆâ–‹       | 2000/7540 [00:00<00:01, 3146.11 examples/s]Map:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 3000/7540 [00:01<00:01, 2758.82 examples/s]Map:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 4000/7540 [00:01<00:00, 3756.45 examples/s]Map:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 5000/7540 [00:01<00:00, 3116.70 examples/s]Map:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 6000/7540 [00:01<00:00, 2836.38 examples/s]Map:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 7000/7540 [00:02<00:00, 3609.80 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7540/7540 [00:02<00:00, 2764.72 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7540/7540 [00:02<00:00, 3068.10 examples/s]
Map:   0%|          | 0/5028 [00:00<?, ? examples/s]Map:  20%|â–ˆâ–‰        | 1000/5028 [00:00<00:01, 2383.10 examples/s]Map:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 2000/5028 [00:00<00:00, 4122.37 examples/s]Map:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3000/5028 [00:01<00:01, 1843.94 examples/s]Map:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4000/5028 [00:01<00:00, 1979.93 examples/s]Map:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5000/5028 [00:02<00:00, 2689.05 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5028/5028 [00:02<00:00, 2475.27 examples/s]
/Users/923673423/lime/environments/lime_env_39/lib64/python3.9/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/Users/923673423/lime/daproject/scripts/gpt-model.py:90: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/708 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/Users/923673423/lime/daproject/scripts/gpt-model.py", line 101, in <module>
    trainer.train()
  File "/Users/923673423/lime/environments/lime_env_39/lib64/python3.9/site-packages/transformers/trainer.py", line 2164, in train
    return inner_training_loop(
  File "/Users/923673423/lime/environments/lime_env_39/lib64/python3.9/site-packages/transformers/trainer.py", line 2522, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
  File "/Users/923673423/lime/environments/lime_env_39/lib64/python3.9/site-packages/transformers/trainer.py", line 3655, in training_step
    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
  File "/Users/923673423/lime/environments/lime_env_39/lib64/python3.9/site-packages/transformers/trainer.py", line 3709, in compute_loss
    outputs = model(**inputs)
  File "/Users/923673423/lime/environments/lime_env_39/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/923673423/lime/environments/lime_env_39/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/923673423/lime/environments/lime_env_39/lib64/python3.9/site-packages/torch/nn/parallel/data_parallel.py", line 193, in forward
    outputs = self.parallel_apply(replicas, inputs, module_kwargs)
  File "/Users/923673423/lime/environments/lime_env_39/lib64/python3.9/site-packages/torch/nn/parallel/data_parallel.py", line 212, in parallel_apply
    return parallel_apply(
  File "/Users/923673423/lime/environments/lime_env_39/lib64/python3.9/site-packages/torch/nn/parallel/parallel_apply.py", line 126, in parallel_apply
    output.reraise()
  File "/Users/923673423/lime/environments/lime_env_39/lib64/python3.9/site-packages/torch/_utils.py", line 715, in reraise
    raise exception
AssertionError: Caught AssertionError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/Users/923673423/lime/environments/lime_env_39/lib64/python3.9/site-packages/torch/nn/parallel/parallel_apply.py", line 96, in _worker
    output = module(*input, **kwargs)
  File "/Users/923673423/lime/environments/lime_env_39/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/923673423/lime/environments/lime_env_39/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/923673423/lime/environments/lime_env_39/lib64/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 1607, in forward
    assert (
AssertionError: Cannot handle batch sizes > 1 if no padding token is defined.

  0%|          | 0/708 [00:05<?, ?it/s]
